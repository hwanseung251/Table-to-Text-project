{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjG2lNI_LCj5"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "# 필요한 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# 모델 로드 및 가중치 적용\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/프로젝트NLP/학습모델weight/best_model.pth\"))\n",
        "model.to(device)\n",
        "\n",
        "class_weights = torch.load(\"/content/drive/MyDrive/프로젝트NLP/학습모델weight/class_weights.pth\").to(device)"
      ],
      "metadata": {
        "id": "7xq1D4BMLHpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "\n",
        "# LIME 설명자를 생성합니다.\n",
        "explainer = LimeTextExplainer(class_names=[\"no\", \"yes\"])\n",
        "\n",
        "#gpu터져서 cpu사용\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 모델의 예측 함수를 정의합니다.\n",
        "def predictor(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=160, return_tensors=\"pt\")\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1).detach().cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "lime_summary = []\n",
        "for i, text in enumerate(test['text'][1000:]):\n",
        "    exp = explainer.explain_instance(\n",
        "        text,\n",
        "        predictor,\n",
        "        num_features=10,\n",
        "        num_samples=100\n",
        "    )\n",
        "    feature_contributions = \", \".join([f\"{feature}:{round(contribution, 3)}\" for feature, contribution in exp.as_list()])\n",
        "\n",
        "    lime_summary.append({\n",
        "        \"text\":text,\n",
        "        \"feature_contributions\": feature_contributions\n",
        "    })\n",
        "\n",
        "lime_summary_df = pd.DataFrame(lime_summary)\n",
        "# lime_summary_df.to_excel('/content/drive/MyDrive/프로젝트NLP/lime_summary_test2.xlsx', index=False)"
      ],
      "metadata": {
        "id": "PrHGeLuOLHmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}