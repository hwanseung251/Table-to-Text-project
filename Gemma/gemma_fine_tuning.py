# -*- coding: utf-8 -*-
"""gemma_fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cyj44K7RrzertrJPnL0C_95qkNux1hT8
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
summary = pd.read_csv('/kaggle/input/summary-new/summary_new.csv')
summary.head()

summary['text'][0]

summary['feature_contributions'][0]

summary['y'][0]

summary=summary[["text","analysis"]]
summary.head()

summary = summary.reset_index()

summary = summary[['index','analysis']]
summary.rename(columns={'index':'user_name','analysis':'text'}, inplace=True)
summary.head()

import nltk
import re
nltk.download('stopwords')
stemmer = nltk.SnowballStemmer("english")
from nltk.corpus import stopwords
import string
stopword=set(stopwords.words('english'))

# Regular Expressions for text cleaning
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')  # Keep lowercase letters, digits, and some special characters
STOPWORDS = set(stopwords.words('english'))

# function to clean text
def clean_text(text):
    """
    text: a string
    return: cleaned and preprocessed string
    """
    text = str(text).lower()  # Convert text to lowercase
    text = REPLACE_BY_SPACE_RE.sub(' ', text)  # Replace certain symbols with a space
    text = BAD_SYMBOLS_RE.sub('', text)  # Remove unwanted symbols
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)  # Remove stopwords
    return text


summary['cleaned_text'] = summary['text'].apply(clean_text)

# verify cleaning process
print(summary[['text', 'cleaned_text']].head())

summary['text'][0]

review_data = summary.apply(lambda x: f"{x['text']}", axis=1).tolist()
review_data[:10]

!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117
!pip install -q -U -i https://pypi.org/simple/ bitsandbytes
!pip install -q -U transformers
!pip install -q -U accelerate
!pip install -q -U datasets
!pip install -q -U trl
!pip install -q -U peft
!pip install -q -U wikipedia-api

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import warnings
warnings.filterwarnings("ignore")

import re
import numpy as np
import pandas as pd
from tqdm import tqdm
import wikipediaapi

import torch

import numpy as np
import pandas as pd
import os
from tqdm import tqdm

import torch
import torch.nn as nn

import transformers
from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          TrainingArguments,
                          )

from datasets import Dataset
from peft import LoraConfig, PeftConfig
import bitsandbytes as bnb
from trl import SFTTrainer

def define_device():
    """Define the device to be used by PyTorch"""

    # Get the PyTorch version
    torch_version = torch.__version__

    # Print the PyTorch version
    print(f"PyTorch version: {torch_version}", end=" -- ")

    # Check if MPS (Multi-Process Service) device is available on MacOS
    if torch.backends.mps.is_available():
        # If MPS is available, print a message indicating its usage
        print("using MPS device on MacOS")
        # Define the device as MPS
        defined_device = torch.device("mps")
    else:
        # If MPS is not available, determine the device based on GPU availability
        defined_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # Print a message indicating the selected device
        print(f"using {defined_device}")

    # Return the defined device
    return defined_device

extracted_texts = review_data

import kagglehub

kagglehub.model_download('google/gemma/Transformers/2b-it/3')

model_name = "/kaggle/input/gemma/transformers/2b-it/3"

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

max_seq_length = 512
tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)

def question_gemma(question, model=model, tokenizer=tokenizer, temperature=0.0, return_answer=False):
    input_ids = tokenizer(question, return_tensors="pt").to("cuda")
    if temperature > 0:
        do_sample=True
    else:
        do_sample=False
    outputs = model.generate(**input_ids,
                             max_new_tokens=512,
                             do_sample=do_sample,
                             temperature=temperature)
    result = str(tokenizer.decode(outputs[0])).replace("<bos>", "").replace("<eos>", "").strip()
    if return_answer:
        return result
    else:
        print(result)

question_gemma("""data = {'information':,'This customer is 42 years old, works as a technician, and is married. They have completed professional_course education, have no credit issues, and do not have a housing or personal loan. The previous contact was made by cellular, with the most recent contact occurring on a thu in aug. The customer has never been contacted before and did not participate in previous marketing campaigns. At that time, the quarterly employment change rate was -1.7%, the consumer price index was 94.027, the consumer confidence index was -38.3, the EURIBOR 3-month rate was 0.899, and the employee count was 4991.6.',
'text_importances':'1:0.028, was:0.025, never:0.02, At:0.016, most:0.015, thu:0.015, did:0.009, is:-0.004, 94:0.002, EURIBOR:0.001',
'prediction':'yes'}

The following is the text importance contributed by this prediction model in predicting whether to subscribe to a term deposit in the prediction model using customer information in the 'text' column and data in the 'text' column entered in 'text_importances'. ' heat. And ‘prediction’ is the model’s prediction value about whether to sign up for a term deposit. Summarize and analyze this information. Based on your analysis, if you predict that customers will sign up for a term deposit, recommend a marketing strategy to use to keep those customers enrolled.""")

review_data[0]

question_gemma("""This customer did not sign up for a term deposit. This customer's information is as follows : Employees must have completed basic_4y education, have no credit issues, and do not have a housing or personal loan. At that time, the quarterly employment change rate was 1.

Here's some information about whether this customer will sign up regularly and the big influencing factors that will influence whether or not they sign up. Based on this information, analyze this customer's information by reminding them of their subscription and connection.""")

qa_data = []

def extract_json(text, word):
    pattern = fr'"{word}": "(.*?)"'
    match = re.search(pattern, text)
    if match:
        return match.group(1)
    else:
        return ""

no_extracted_texts = 300 # increment this number up to len(extracted_texts)
question_ratio = 24 # decrement this number to produce more questions (suggested: 24)

for i in tqdm(range(len(extracted_texts[:no_extracted_texts]))):

    question_text = f"""
Next, questions and answers are prepared based on customer information regarding whether or not to subscribe to a term deposit. Make sure your question includes all the details relevant to the information so it's clear to anyone who hasn't read it. The answer must accurately reflect whether the customer has subscribed to a term deposit and any key points mentioned in the information. Specific words that influenced whether to sign up for a term deposit and the content of sentences containing those words must be analyzed and reflected in the ranking or importance of the information that influenced whether to sign up.
Return the result exclusively in JSON format as follows: {'{"question": "...", "answer": "..."}'}

    Here is the consumer information:
    {extracted_texts[i]}

    OUTPUT JSON:
    """

    no_questions = min(1, len(extracted_texts[i]) // question_ratio)
    for j in range(no_questions):

        result = question_gemma(question_text, model=model, temperature=0.9, return_answer=True)
        result = result.split("OUTPUT JSON:")[-1]

        question = extract_json(result, "question")
        answer = extract_json(result, "answer")

        qa_data.append(f"{question}\n{answer}")

from sklearn.model_selection import train_test_split
max_seq_length = 128

train_data = (pd.DataFrame(qa_data, columns=["text"])
              .sample(frac=1, random_state=5)
              .drop_duplicates()
             )
train_data = Dataset.from_pandas(train_data)

output_dir = "gemma_Informations"

peft_config = LoraConfig(
    #lora_r: int = 32
    #lora_alpha: float = lora_r * 2
    lora_alpha=64,
    lora_dropout=0.01,
    r=16,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj']
    )

training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=1,
    gradient_checkpointing=True,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=8,
    optim="paged_adamw_32bit",
    save_steps=0,
    logging_steps=25,
    learning_rate=1e-4,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=False,
    evaluation_strategy='no',
    eval_steps = 500,
    eval_accumulation_steps=1,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    peft_config=peft_config,
    dataset_text_field="text",
    tokenizer=tokenizer,
    max_seq_length=max_seq_length,
    args=training_arguments,
    packing=False,
)

trainer.train()

trainer.save_model()
tokenizer.save_pretrained(output_dir)

import gc

del [model, tokenizer, peft_config, trainer, train_data, bnb_config, training_arguments]
del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]

for _ in range(10):
    torch.cuda.empty_cache()
    gc.collect()

from peft import AutoPeftModelForCausalLM

finetuned_model = output_dir
compute_dtype = getattr(torch, "float16")
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoPeftModelForCausalLM.from_pretrained(
     finetuned_model,
     torch_dtype=compute_dtype,
     return_dict=False,
     low_cpu_mem_usage=True,
     device_map="auto",
)

merged_model = model.merge_and_unload()
merged_model.save_pretrained("./gemma_Informations_merged",
                             safe_serialization=True,
                             max_shard_size="2GB")
tokenizer.save_pretrained("./gemma_Informations_merged")

import gc

del [model, tokenizer, merged_model, AutoPeftModelForCausalLM]

for _ in range(10):
    torch.cuda.empty_cache()
    gc.collect()

for _ in range(10):
    torch.cuda.empty_cache()
    gc.collect()

from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig)

model_name = "/kaggle/working/gemma_Informations_merged"

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

max_seq_length = 512
tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)

question_gemma("Please summarize and explain the characteristics of customers who sign up for a term deposit.",
                model=model, tokenizer=tokenizer)

question_gemma(""""The word '1' (absolute impact: 0.028) is included in the following sentence, supporting the customer's decision to subscribe: 'At that time, the quarterly employment change rate was -1'. The word 'was' (absolute impact: 0.025) is included in the following sentence, supporting the customer's decision to subscribe: 'The previous contact was made by cellular, with the most recent contact occurring on a thu in aug'. The word 'never' (absolute impact: 0.02) is included in the following sentence, supporting the customer's decision to subscribe: 'The customer has never been contacted before and did not participate in previous marketing campaigns'. The word 'At' (absolute impact: 0.016) is included in the following sentence, supporting the customer's decision to subscribe: 'At that time, the quarterly employment change rate was -1'. The word 'most' (absolute impact: 0.015) is included in the following sentence, supporting the customer's decision to subscribe: 'The previous contact was made by cellular, with the most recent contact occurring on a thu in aug'. The word 'thu' (absolute impact: 0.015) is included in the following sentence, supporting the customer's decision to subscribe: 'The previous contact was made by cellular, with the most recent contact occurring on a thu in aug'. The word 'did' (absolute impact: 0.009) is included in the following sentence, supporting the customer's decision to subscribe: 'The customer has never been contacted before and did not participate in previous marketing campaigns'. The word 'is' (absolute impact: 0.004) is included in the following sentence, supporting the customer's decision to subscribe: 'This customer is 42 years old, works as a technician, and is married'. The word '94' (absolute impact: 0.002) is included in the following sentence, supporting the customer's decision to subscribe: '7%, the consumer price index was 94'. The word 'EURIBOR' (absolute impact: 0.001) is included in the following sentence, supporting the customer's decision to subscribe: '3, the EURIBOR 3-month rate was 0'."
Here's some information about whether this customer will sign up regularly and the big influencing factors that will influence whether or not they sign up. Based on this information, analyze this customer's information by reminding them of their subscription and connection.""",
                model=model, tokenizer=tokenizer)

