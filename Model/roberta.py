# -*- coding: utf-8 -*-
"""roberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12gRrsF1l-sEqp-U-hNEDwys-Yb7HC2Kx
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
import re
from sklearn.utils.class_weight import compute_class_weight
import random

"""data load"""

train_원본 = pd.read_excel('/content/drive/MyDrive/프로젝트NLP/train_원본.xlsx')
train_aug = pd.read_excel('/content/drive/MyDrive/프로젝트NLP/train_aug.xlsx')
val = pd.read_excel('/content/drive/MyDrive/프로젝트NLP/val.xlsx')
test = pd.read_excel('/content/drive/MyDrive/프로젝트NLP/test.xlsx')

train_aug = train_aug.rename(columns={'text_aug':'text'})
concat2 = pd.concat([train_원본, train_aug]).reset_index(drop=True)

train_back = pd.read_excel('/content/drive/MyDrive/프로젝트NLP/train_backaug.xlsx')
final = pd.concat([concat2, train_back]).reset_index(drop=True)

final_shuffled = final.sample(frac=1, random_state=42).reset_index(drop=True)

"""roberta classification"""

import torch
import os
from torch.utils.data import DataLoader, Dataset
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import classification_report, f1_score, matthews_corrcoef
from transformers import DataCollatorForLanguageModeling
from torch.optim import AdamW

# 1. 데이터셋 정의
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]

        if isinstance(text, list):
          text = f" {self.tokenizer.sep_token} ".join(text)

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

"""'learning_rate': 0.000003, 'batch_size': 32, 'weight_decay': 0

Best model saved with F1 score: 0.5071
Train loss 0.5239019664601214 F1 score 0.49255708004449866 MCC 04254117356247446
Val loss 0.5250794845958089 F1 score 0.5070671378091872 MCC 0.44111638375736484
"""

train_texts = final_shuffled['text']
val_texts = val['text']
train_labels = final_shuffled['y']
val_labels = val['y']

label_mapping = {'yes': 1, 'no': 0}

train_labels = final_shuffled['y'].map(label_mapping)
val_labels = val['y'].map(label_mapping)

# 로버타 토크나이저 로드
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# 데이터셋 생성
train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=160)
val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_len=160)


# 데이터로더 생성
train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle = False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# 3. 모델 정의
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 4. 옵티마이저 및 손실 함수 정의
optimizer = AdamW(model.parameters(), lr=0.000003)

# 가중치를 적용한 손실 함수 정의
loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

# 5. 모델 학습
def train_epoch(model, dataloader, optimizer, device, loss_fn):
    model.train()
    losses = []
    y_true = []
    y_preds = []

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits

        loss = loss_fn(logits, labels)  # 가중치가 적용된 손실 함수 사용
        _, preds = torch.max(logits, dim=1)
        y_true.extend(labels.cpu().numpy())
        y_preds.extend(preds.cpu().numpy())
        losses.append(loss.item())

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    avg_f1 = f1_score(y_true, y_preds, average='binary')
    avg_mcc = matthews_corrcoef(y_true, y_preds)  # MCC 추가
    return avg_f1, avg_mcc, np.mean(losses)

# 6. 모델 평가
def eval_model(model, dataloader, device, loss_fn):
    model.eval()
    losses = []
    y_true = []
    y_preds = []
    y_probas = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            loss = loss_fn(logits, labels)  # 가중치가 적용된 손실 함수 사용
            _, preds = torch.max(logits, dim=1)
            y_true.extend(labels.cpu().numpy())
            y_preds.extend(preds.cpu().numpy())
            losses.append(loss.item())

            proba = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
            y_probas.extend(proba)

    avg_f1 = f1_score(y_true, y_preds, average='binary')
    avg_mcc = matthews_corrcoef(y_true, y_preds)  # MCC 추가
    return avg_f1, avg_mcc, np.mean(losses), y_preds, y_probas


# 모델 저장 경로 설정
best_model_path = "best_model.pth"
best_f1_score = 0.0  # 가장 높은 F1 score를 기록

epochs = 10

for epoch in range(epochs):
    print(f'Epoch {epoch + 1}/{epochs}')
    print('-' * 10)

    train_f1, train_mcc, train_loss = train_epoch(model, train_dataloader, optimizer, device, loss_fn)
    print(f'Train loss {train_loss} F1 score {train_f1} MCC {train_mcc}')

    val_f1, val_mcc, val_loss, val_pred, val_proba = eval_model(model, val_dataloader, device, loss_fn)
    print(f'Val loss {val_loss} F1 score {val_f1} MCC {val_mcc}')

    if val_f1 > best_f1_score:
        best_f1_score = val_f1
        torch.save(model.state_dict(), best_model_path)
        print(f"Best model saved with F1 score: {best_f1_score:.4f}")

print(f"Loading best model with F1 score: {best_f1_score:.4f}")
model.load_state_dict(torch.load(best_model_path))

test_texts = test['text']
test_labels = test['y']

test_labels = test['y'].map(label_mapping)

test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_len=160)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)


test_f1, test_mcc, test_loss, test_pred, test_proba = eval_model(model, test_dataloader, device, loss_fn)
print(f'test loss {test_loss} F1 score {test_f1} MCC {test_mcc}')

val_results = pd.DataFrame({
    'true_label': val_labels,  # 실제 레이블
    'predicted_label': val_pred,  # 예측 레이블
    'predicted_proba': val_proba  # 예측 확률
})
val_results.to_csv('/content/drive/MyDrive/프로젝트NLP/model_predictions/roberta_val_predictions2.csv', index=False)

# 테스트 데이터 예측값과 예측 확률을 CSV로 저장
test_results = pd.DataFrame({
    'true_label': test_labels,  # 실제 레이블
    'predicted_label': test_pred,  # 예측 레이블
    'predicted_proba': test_proba  # 예측 확률
})
test_results.to_csv('/content/drive/MyDrive/프로젝트NLP/model_predictions/roberta_test_predictions2.csv', index=False)

print("CSV files for val and test predictions saved!")

model_path = "/content/drive/MyDrive/프로젝트NLP/학습모델weight/best_model.pth"
class_weights_path = "/content/drive/MyDrive/프로젝트NLP/학습모델weight/class_weights.pth"

torch.save(model.state_dict(), model_path)
torch.save(class_weights, class_weights_path)

# # Google Drive에서 파일 로드
# model.load_state_dict(torch.load("/content/drive/MyDrive/best_model.pth"))
# class_weights = torch.load("/content/drive/MyDrive/class_weights.pth").to(device)









