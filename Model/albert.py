# -*- coding: utf-8 -*-
"""ALBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kc4xEyFEJ2qCNej1X5u07X3JMo9Vno5b

##필요한 라이브러리 임포트
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, matthews_corrcoef
import numpy as np
from transformers import AlbertForSequenceClassification, Trainer, TrainingArguments
import torch
from transformers import AlbertTokenizer
from torch.utils.data import Dataset
from sklearn.utils.class_weight import compute_class_weight
import torch.nn as nn
from sklearn.utils import shuffle
from transformers import get_linear_schedule_with_warmup
from transformers import EarlyStoppingCallback
from google.colab import drive
drive.mount('/content/drive')
#파일 경로 설정
root = '/content/drive/MyDrive/국민대학교/D&A Conference/data/데이터 최종/'

"""### 데이터 로드"""

train_data = pd.read_excel(root+"train_data.xlsx")
train_backaug = pd.read_excel(root+"train_backaug.xlsx")
train_original = pd.read_excel(root+"train_원본.xlsx")
train_data.drop(columns='Unnamed: 0', inplace=True); train_data

p = inflect.engine()

def convert_numbers_to_words(text):
    # 숫자를 탐색하는 정규 표현식
    def replace_func(match):
        number = int(match.group())
        return p.number_to_words(number)  # 숫자를 영어 단어로 변환

    # 텍스트에서 숫자를 찾아 변환
    return re.sub(r'\d+', replace_func, text)

# 예시 데이터
train_original['text'] = [convert_numbers_to_words(sentence) for sentence in train_original['text']]

train_data2 = pd.concat([train_data, train_backaug, train_original], axis=0, ignore_index=True)

train_data = train_data2

val_data = pd.read_excel(root + 'val_data.xlsx')

test_data = pd.read_excel(root + 'test_data.xlsx')

train_data = train_data.drop(columns='Unnamed: 0')
print(train_data['y'].value_counts())

"""### 데이터 인코딩"""

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', clean_up_tokenization_spaces=True)

def encode_data(data, tokenizer, max_length=160):
    return tokenizer(
        data['text'].tolist(),
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )

#인코딩
train_encodings = encode_data(train_data, tokenizer)
val_encodings = encode_data(val_data, tokenizer)
test_encodings = encode_data(test_data, tokenizer)

#라벨->텐서
train_labels = torch.tensor(train_data['y'].apply(lambda x: 1 if x == 'yes' else 0).tolist(), dtype=torch.long)
val_labels = torch.tensor(val_data['y'].apply(lambda x: 1 if x == 'yes' else 0).tolist(), dtype=torch.long)
test_labels = torch.tensor(test_data['y'].apply(lambda x: 1 if x == 'yes' else 0).tolist(), dtype=torch.long)

"""### 평가지표"""

def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)

    f1_scores = f1_score(labels, preds, average=None)

    # Class_1에 대한 F1 score 추출
    f1_class_1 = f1_scores[1]

    mcc = matthews_corrcoef(labels, preds)

    return {
        'f1_label: 1': f1_class_1,
        'mcc': mcc,
        'eval_loss': p[0].mean()
    }

"""### 데이터셋으로 변환"""

class CustomerDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CustomerDataset(train_encodings, train_labels)
train_dataset = shuffle(train_dataset, random_state=42)
val_dataset = CustomerDataset(val_encodings, val_labels)
test_dataset = CustomerDataset(test_encodings, test_labels)

"""### class_weight 정의 및 커스텀 Trainer 정의"""

classes = np.array(['no', 'yes'])
class_weights = [1.0, 3.0]
device = torch.device("cuda")
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)


#class_weight 대신 사용한 Focal_loss 클래스- 사용하려면 custom trainer 클래스에서도 변경 필요
'''
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, labels):
        ce_loss = nn.CrossEntropyLoss(reduction='none')(logits, labels)
        pt = torch.exp(-ce_loss)  # 확률 예측값
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss  # Focal Loss 공식

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss
'''

# 커스텀 Trainer 클래스 정의
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)  # 가중치 적용된 손실 함수
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

print("Class weights:", class_weights)

"""### 모델 하이퍼 파리미터 설정 및 훈련"""

# 모델 초기화
model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)

# 훈련 인자 설정
training_args = TrainingArguments(
    output_dir='./results',
    report_to="none",
    num_train_epochs=10,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=62,
    gradient_accumulation_steps=4,
    label_smoothing_factor=0.05,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    learning_rate=4e-5,
    lr_scheduler_type="linear" #cosine
)

# Trainer 설정
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# 모델 학습
trainer.train()

# 평가 결과 출력 (평가 후 최종 성능 확인)
eval_results = trainer.evaluate(eval_dataset=val_dataset)
print(f"Validation Loss: {eval_results['eval_loss']}")
print(f"Validation F1: {eval_results['eval_f1_label: 1']}")
print(f"Validation MCC: {eval_results['eval_mcc']}")

test_results = trainer.evaluate(eval_dataset=test_dataset)
print(f"Test Loss: {test_results['eval_loss']}")
print(f"Test F1: {test_results['eval_f1_label: 1']}")
print(f"Test MCC: {test_results['eval_mcc']}")

"""### 모델 저장"""

save_path = '/content/drive/MyDrive/국민대학교/D&A Conference/data/데이터 최종/12data_1:3_modeling_model.pth'

# 모델을 CPU 텐서로 변환해 저장
torch.save(model.to('cpu').state_dict(), save_path)

!du -hsc "{save_path}"